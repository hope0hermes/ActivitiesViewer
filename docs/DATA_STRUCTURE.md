# ActivitiesViewer Data Structure Guide

## Expected Data Directory Structure

ActivitiesViewer expects data from StravaAnalyzer in the following structure:

```
data_enriched/
├── activities_enriched.csv       # Main enriched activities data
├── activity_summary.json         # Aggregate statistics
└── Streams/                      # Individual activity stream files
    ├── stream_1001301785.csv
    ├── stream_10017325050.csv
    ├── stream_10017325928.csv
    └── ... (one CSV per activity)
```

### File Specifications

#### 1. `activities_enriched.csv`

**Description:** Main CSV file containing enriched metrics for all activities.

**Generated by:** StravaAnalyzer (`strava-analyzer process`)

**Size:** Varies (1,812 activities in the example = ~140 columns × ~1,800 rows)

**Key Columns Expected:**
- `start_date` - Activity start date/time
- `activity_id` - Unique Strava activity identifier
- `distance` - Distance in meters
- `moving_time` - Time spent moving in seconds
- `total_elevation_gain` - Elevation gain in meters
- `normalized_power` / `moving_normalized_power` - NP in watts
- `intensity_factor` / `moving_intensity_factor` - IF (0-1 scale)
- `moving_training_stress_score` - TSS score
- `average_hr` / `moving_average_hr` - Average heart rate
- `moving_power_z1_percentage` through `moving_power_z7_percentage` - Power zone percentages
- `moving_hr_z1_percentage` through `moving_hr_z5_percentage` - HR zone percentages
- **NEW Advanced Power Metrics:**
  - `time_above_90_ftp` / `moving_time_above_90_ftp` - Seconds above 90% FTP
  - `time_sweet_spot` / `moving_time_sweet_spot` - Seconds in 88-94% FTP (sweet spot)
  - `w_prime_balance_min` / `moving_w_prime_balance_min` - Minimum W' balance reached
  - `match_burn_count` / `moving_match_burn_count` - Number of significant W' expenditures
  - `negative_split_index` / `moving_negative_split_index` - NP 2nd half / NP 1st half
  - `cardiac_drift` / `moving_cardiac_drift` - EF 1st half vs 2nd half (%)
  - `estimated_ftp` / `moving_estimated_ftp` - FTP estimate from 20min power
- **NEW Climbing Metrics:**
  - `vam` / `moving_vam` - Velocità Ascensionale Media (m/h)
  - `climbing_time` / `moving_climbing_time` - Time spent climbing (seconds)
  - `climbing_power` / `moving_climbing_power` - Avg power on climbs >4% (W)
  - `climbing_power_per_kg` / `moving_climbing_power_per_kg` - Climbing W/kg
- Additional advanced metrics (VI, EF, decoupling, etc.)

**Example Usage:**
```python
import pandas as pd

activities_df = pd.read_csv("activities_enriched.csv", parse_dates=["start_date"])
print(f"Total activities: {len(activities_df)}")
print(f"Columns: {list(activities_df.columns)}")
print(f"Date range: {activities_df['start_date'].min()} to {activities_df['start_date'].max()}")
```

#### 2. `activity_summary.json`

**Description:** JSON file containing aggregate statistics and summaries.

**Generated by:** StravaAnalyzer (`strava-analyzer process`)

**Structure:** Flat or nested dictionary with aggregate metrics

**Example Content:**
```json
{
  "total_activities": 1812,
  "total_distance_km": 37931.2,
  "total_time_hours": 1089.5,
  "average_distance_km": 20.9,
  "current_ctl": 51.5,
  "current_atl": 18.3,
  "current_tsb": 33.2,
  "recent_performance": {
    "ftp_estimate": 285,
    "30min_power": 242,
    "5min_power": 315
  }
}
```

**Usage:**
```python
import json

with open("activity_summary.json") as f:
    summary = json.load(f)

print(f"Total activities: {summary['total_activities']}")
print(f"CTL: {summary['current_ctl']}")
```

#### 3. `Streams/stream_*.csv`

**Description:** Individual activity stream files with time-series data.

**Naming Convention:** `stream_{activity_id}.csv`

**Generated by:** StravaAnalyzer (one per activity)

**Typical Columns:**
- `time` - Time in seconds from activity start
- `moving` - Boolean flag for moving/stopped
- `watts` (optional) - Power in watts
- `heartrate` (optional) - Heart rate in bpm
- `cadence` (optional) - Cadence in RPM
- `distance` (optional) - Distance in meters
- `grade_smooth` (optional) - Gradient percentage
- `altitude` (optional) - Altitude in meters
- `velocity_smooth` (optional) - Speed in m/s
- `latlng` (optional) - GPS coordinates [lat, lng]

**Example File:** `stream_1234567890.csv`
```csv
time,moving,watts,heartrate,cadence,distance,altitude,velocity_smooth
0,False,0,0,0,0,120.5,0
1,True,85,110,85,2.1,120.6,2.1
2,True,92,112,86,4.2,120.7,2.1
...
```

**Usage:**
```python
import pandas as pd

activity_id = 1234567890
stream_path = f"Streams/stream_{activity_id}.csv"
stream_df = pd.read_csv(stream_path)

print(f"Activity duration: {stream_df['time'].max()} seconds")
print(f"Average power: {stream_df['watts'].mean():.0f} W")
print(f"Max power: {stream_df['watts'].max():.0f} W")
```

## Configuration Setup

Create a `config.yaml` file pointing to your data:

```yaml
# Absolute path to data directory
data_dir: "/path/to/data_enriched"

# Or relative path (resolved from config file location)
data_dir: "../dev/data_enriched"

# Filenames within data_dir
activities_enriched_file: "activities_enriched.csv"
activity_summary_file: "activity_summary.json"
streams_dir: "Streams"

# Athlete settings
ftp: 285.0          # Your FTP in watts
weight_kg: 77.0     # Your weight in kg
max_hr: 185         # Your max HR in bpm
```

## Validation Checklist

Before running ActivitiesViewer, verify:

```bash
# 1. Check directory structure
ls -la ~/data_enriched/
# Should show: activities_enriched.csv, activity_summary.json, Streams/

# 2. Check CSV file
head -5 ~/data_enriched/activities_enriched.csv
wc -l ~/data_enriched/activities_enriched.csv  # Should match expected count

# 3. Check JSON file
cat ~/data_enriched/activity_summary.json | head -20

# 4. Check streams directory
ls ~/data_enriched/Streams/ | head -10
ls ~/data_enriched/Streams/ | wc -l  # Should match activity count

# 5. Validate with CLI
activities-viewer validate --config config.yaml
```

## Data Import from StravaAnalyzer

### Complete Workflow

```bash
# 1. Generate enriched data with StravaAnalyzer
cd ../StravaAnalyzer
uv run strava-analyzer process --config config.yaml

# Output files created:
# - activities_enriched.csv
# - activity_summary.json
# - (streams already exist)

# 2. Copy output to ActivitiesViewer
cp -r output_dir/data_enriched ../ActivitiesViewer/dev/

# 3. Create ActivitiesViewer config
cd ../ActivitiesViewer
cp examples/config.yaml config.yaml

# 4. Update paths in config.yaml
# data_dir: "../dev/data_enriched"

# 5. Validate and run
activities-viewer validate --config config.yaml
activities-viewer run --config config.yaml
```

### Expected Outputs from StravaAnalyzer

When you run StravaAnalyzer, it produces:

1. **activities_enriched.csv** - Original activities + calculated metrics
2. **daily_summary.csv** - Optional daily aggregate (not used by ActivitiesViewer yet)
3. **Streams/** - Individual stream files (copied/created from original data)

## Troubleshooting Data Issues

### Missing or Empty Files

```bash
# Check file size
du -h activities_enriched.csv

# Check CSV structure
head -1 activities_enriched.csv  # Check columns
wc -l activities_enriched.csv    # Check row count

# Check JSON validity
python3 -m json.tool activity_summary.json | head -20
```

### Stream File Issues

```bash
# List stream files
ls Streams/stream_*.csv | wc -l  # Count streams

# Check a stream file
head -5 Streams/stream_1234567890.csv
wc -l Streams/stream_1234567890.csv

# Find empty streams
find Streams -size 0 -name "*.csv"
```

### Data Type Issues

```python
# Verify data types
import pandas as pd

df = pd.read_csv("activities_enriched.csv")
print(df.dtypes)

# Check for missing values
print(df.isnull().sum())

# Check date parsing
df['start_date'] = pd.to_datetime(df['start_date'])
print(f"Date range: {df['start_date'].min()} to {df['start_date'].max()}")
```

## Data Updates

### Refreshing Data from Strava

1. Run StravaAnalyzer again to fetch new activities
2. Replace the CSV and Streams files
3. Restart ActivitiesViewer dashboard

```bash
# Update workflow
cd ../StravaAnalyzer
uv run strava-analyzer run --config config.yaml  # Fetch new data

cd ../ActivitiesViewer
# Dashboard will auto-reload with new data (if using hot reload)
```

## Performance Notes

- **Large Datasets:** With 1,812+ activities, CSV loading is cached in Streamlit
- **Stream Files:** Individual streams are loaded on-demand when viewing activity details
- **Memory:** Full dataset in memory (~500MB+ for large activity counts)
- **Caching:** Configure `cache_ttl` in config.yaml (default: 3600 seconds)

## Integration with Other Tools

### Exporting Data

```python
# Export filtered activities
import pandas as pd

df = pd.read_csv("activities_enriched.csv")
recent = df[df['start_date'] > '2025-01-01']
recent.to_csv("recent_activities.csv", index=False)
```

### Combining with External Data

```python
# Add external metrics (weather, elevation data, etc.)
external = pd.read_csv("weather_data.csv")
df = pd.merge(df, external, on=['start_date', 'activity_id'])
df.to_csv("activities_enriched_with_weather.csv", index=False)
```

## Next Steps

1. Verify data structure matches this guide
2. Create `config.yaml` with correct paths
3. Run `activities-viewer validate --config config.yaml`
4. Launch dashboard: `activities-viewer run --config config.yaml`
5. Navigate to `http://localhost:8501`

## Reference

- [StravaAnalyzer](https://github.com/hope0hermes/StravaAnalyzer) - Data generation
- [CLI & Configuration Guide](CLI_CONFIGURATION.md) - CLI reference
- [Setup Guide](SETUP.md) - Environment setup
- [Implementation Plan](../DASHBOARD_IMPLEMENTATION_PLAN.md) - Features and roadmap
